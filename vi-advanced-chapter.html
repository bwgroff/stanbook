<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>10 Variational Inference | Bayesian Workflow Using Stan</title>
  <meta name="description" content="10 Variational Inference | Bayesian Workflow Using Stan, with examples and programming techniques.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="10 Variational Inference | Bayesian Workflow Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="10 Variational Inference | Bayesian Workflow Using Stan, with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Variational Inference | Bayesian Workflow Using Stan" />
  
  <meta name="twitter:description" content="10 Variational Inference | Bayesian Workflow Using Stan, with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayesian-point-estimation.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="part-2-bayesian-inference.html#part-2-bayesian-inference"><i style="font-size: 110%; color:#990017;">Part 2: Bayesian Inference</i></a></li>
<li class="chapter" data-level="7" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>7</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="8" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>8</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="9" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="10" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>10</b> Variational Inference</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Workflow Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vi-advanced.chapter" class="section level1">
<h1><span class="header-section-number">10</span> Variational Inference</h1>
<p>Stan implements an automatic variational inference algorithm based on transforming variables to the unconstrained scale and using a normal approximating distribution.</p>
<p>Classical variational inference algorithms are difficult to derive. We must first define the family of approximating densities, and then calculate model-specific quantities relative to that family to solve the variational optimization problem. Both steps require expert knowledge. The resulting algorithm is tied to both the model and the chosen approximation.</p>
<p>We begin by briefly describing the classical variational inference framework. For a thorough exposition, please refer to <span class="citation">(Jordan et al. <a href="#ref-Jordan:1999">1999</a>; Wainwright and Jordan <a href="#ref-Wainwright-Jordan:2008">2008</a>)</span>; for a textbook presentation, please see <span class="citation">Bishop (<a href="#ref-Bishop:2006">2006</a>)</span>. We follow with a high-level description of automatic differentiation variational inference (ADVI). For more details, see <span class="citation">Kucukelbir et al. (<a href="#ref-Kucukelbir:2015">2015</a>)</span>.</p>
<div id="classical-variational-inference" class="section level2">
<h2><span class="header-section-number">10.1</span> Classical Variational Inference</h2>
<p>Variational inference approximates the posterior <span class="math inline">\(p(\theta \, | \, y)\)</span> with a simple, parameterized distribution <span class="math inline">\(q(\theta \, | \, \phi)\)</span>. It matches the approximation to the true posterior by minimizing the Kullback-Leibler (KL) divergence,</p>
<p><span class="math display">\[
  \phi^* = \mbox{argmin}_\phi
  \mbox{KL}{q(\theta \, | \, \phi) }{ p(\theta \mid y)}.
\]</span></p>
<p>Typically the KL divergence lacks an analytic, closed-form solution. Instead we maximize a proxy to the KL divergence, the so-called “evidence lower bound” (ELBO)</p>
<p><span class="math display">\[
  \mathcal{L} (\phi)
  =
  \mbox{E}_{q (\theta)} ( \log p (y,\theta) )
  -
  \mbox{E}_{q (\theta)} ( \log q (\theta\, | \,\phi) ).
\]</span></p>
<p>The first term is an expectation of the log joint density under the approximation, and the second is the entropy of the variational density. Maximizing the ELBO minimizes the KL divergence <span class="citation">(Jordan et al. <a href="#ref-Jordan:1999">1999</a>; Bishop <a href="#ref-Bishop:2006">2006</a>)</span>.</p>
</div>
<div id="automatic-variational-inference" class="section level2">
<h2><span class="header-section-number">10.2</span> Automatic Variational Inference</h2>
<p>ADVI maximizes the ELBO in the real-coordinate space. Stan transforms the parameters from (potentially) constrained domains to the real-coordinate space. We denote the combined transformation as <span class="math inline">\(T:\theta \to \zeta\)</span>, with the <span class="math inline">\(\zeta\)</span> variables living in <span class="math inline">\(\mathbb{R}^K\)</span>. The variational objective (ELBO) becomes</p>
<p><span class="math display">\[
  \mathcal{L}(\phi)
  =
  \mbox{E}_{q(\zeta\,|\,\phi)}
  \bigg(
  \log p (y, T^{-1}(\zeta))
  +
  \log \big| \det J_{T^{-1}}(\zeta) \big|
  \bigg)
  -
  \mbox{E}_{q (\zeta\, | \,\phi)} \big( \log q (\zeta\, | \,\phi) \big).
\]</span></p>
<p>Since the <span class="math inline">\(\zeta\)</span> variables live in the real-coordinate space, we can choose a fixed family for the variational distribution. We choose a fully-factorized Gaussian,</p>
<p><span class="math display">\[
  q(\zeta \, | \, \phi)
  =
  \mathsf{normal}\left(\zeta \, | \, \mu, \sigma\right)
  =
  \prod_{k=1}^K
  \mathsf{normal}
  \left(\zeta_k \, | \, \mu_k, \sigma_k\right),
\]</span></p>
<p>where the vector <span class="math inline">\(\phi = (\mu_{1},\cdots,\mu_{K}, \sigma_ {1},\cdots,\sigma_{K})\)</span> concatenates the mean and standard deviation of each Gaussian factor. This reflects the “mean-field” assumption in classical variational inference algorithms; we will refer to this particular decomposition as the  option.</p>
<p>The transformation <span class="math inline">\(T\)</span> maps the support of the parameters to the real coordinate space. Thus, its inverse <span class="math inline">\(T^{-1}\)</span> maps back to the support of the latent variables. This implicitly defines the variational approximation in the original latent variable space as</p>
<p><span class="math display">\[
\mathsf{normal} \left(T(\theta) \, | \, \mu, \sigma\right)
| \det J_{T}(\theta) |.
\]</span> This is, in general, not a Gaussian distribution. This choice may call to mind the Laplace approximation technique, where a second-order Taylor expansion around the maximum-a-posteriori estimate gives a Gaussian approximation to the posterior. However, they are not the same <span class="citation">Kucukelbir et al. (<a href="#ref-Kucukelbir:2015">2015</a>)</span>.</p>
<p>The variational objective (ELBO) that we maximize is, <span class="math display">\[
  \mathcal{L}(\phi)
  =
  \mbox{E}_{q(\zeta\, | \,\phi)}
  \bigg(
  \log p (y, T^{-1}(\zeta))
  +
  \log \big| \det J_{T^{-1}}(\zeta) \big|
  \bigg)
  +
  \sum_{k=1}^K \log \sigma_k,
\]</span> where we plug in the analytic form for the Gaussian entropy and drop all terms that do not depend on <span class="math inline">\(\phi\)</span>. The algorithm used for optimization is described in the Stan reference manual chapter on variational inference.</p>

</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Jordan:1999">
<p>Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. 1999. “An Introduction to Variational Methods for Graphical Models.” <em>Machine Learning</em> 37 (2). Springer: 183–233.</p>
</div>
<div id="ref-Wainwright-Jordan:2008">
<p>Wainwright, Martin J, and Michael I Jordan. 2008. “Graphical Models, Exponential Families, and Variational Inference.” <em>Foundations and Trends in Machine Learning</em> 1 (1-2): 1–305.</p>
</div>
<div id="ref-Bishop:2006">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer New York.</p>
</div>
<div id="ref-Kucukelbir:2015">
<p>Kucukelbir, Alp, Rajesh Ranganath, Andrew Gelman, and David M. Blei. 2015. “Automatic Variational Inference in Stan.” <em>arXiv</em> 1506.03431. <a href="http://arxiv.org/abs/1506.03431" class="uri">http://arxiv.org/abs/1506.03431</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-point-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
