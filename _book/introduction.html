<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>1 Introduction | Bayesian Workflow Using Stan</title>
  <meta name="description" content="1 Introduction | Bayesian Workflow Using Stan, with examples and programming techniques.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="1 Introduction | Bayesian Workflow Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="1 Introduction | Bayesian Workflow Using Stan, with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Introduction | Bayesian Workflow Using Stan" />
  
  <meta name="twitter:description" content="1 Introduction | Bayesian Workflow Using Stan, with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="part-1-bayesian-workflow.html">
<link rel="next" href="fake-data-simulation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="part-2-bayesian-inference.html#part-2-bayesian-inference"><i style="font-size: 110%; color:#990017;">Part 2: Bayesian Inference</i></a></li>
<li class="chapter" data-level="7" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>7</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="8" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>8</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="9" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="10" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>10</b> Variational Inference</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Workflow Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="bayesian-data-analysis" class="section level2">
<h2><span class="header-section-number">1.1</span> Bayesian Data Analysis</h2>
<div id="bayesian-inference-and-stan" class="section level3 unnumbered">
<h3>Bayesian inference and Stan</h3>
<p>Bayesian inference is a statistical power tool. It enables you to use prior knowledge and observed data to build a probability model. Your data and unknowns are embedded in this probability model and you construct a “posterior distribution”.</p>
<p>The prior distribution represents the knowledge that you have about the world from past experiments, research or simply scientific truths. The observed data is represented by the ubiquitous likelihood function. We then employ Bayes’ rule to combine the prior and likelihood and approximate the posterior distribution. Once you have this posterior you can make inferences and predictions about everything.</p>
<p>Stan is a platform for statistical modeling and high-performance statistical computation. Given the prior and the data or likelihood function, Stan uses computational methods to approximate the posterior distribution.</p>
<p>A big challenge for applied Bayesian inference is computation and scalability. Converting the mathematical expression of the posterior distribution into specific inferences or predictions (e.g the probability that some coefficient of interest is postive or the 90% predictive interval for some future outcome) can be difficult. For even moderately large or complex problems these quantities are expressed in terms of high-dimensional integrals with no closed-form expressions.</p>
<p>Over the past fifty years, a series of advances in computational statistics have allowed these integrals to be computed using approximations and simulations. The simulations use random numbers and are called “Monte Carlo methods,” named after the city in Europe that is famous for its gambling casinos. These methods were originally developed in the 1940s for aiding in large computations for the military, and in the 1980s it became clear how to apply them for general problems in Bayesian inference.</p>
<p>So: since the 1970s–1980s, methods have been developed to perform approximate computations for Bayesian inferences that would otherwise REQUIRE INTRACTABLE INTERVALS. These approximations needed to be developed one model at a time. In the 1990s–2000s, the WinBugs software was developed, which allowed automatic computation for a large class of Bayesian models. WinBugs (and its successors, OpenBugs and Jags) can be slow, and starting in 2011 we developed Stan, which uses more efficient computations (Hamilton Monte Carlo, the no-U-turn sampler, and algorithmic autodifferentiation) so that automatic Bayesian computation can be applied to larger and more complex problems.</p>
<p>Where we stand now is that, for a fairly broad class of models and data of moderate size, we can transparently program our Bayesian models in Stan and perform inference automatically. This represent the culmination of decades of work in computational statistics, along with corresponding decades of experience fitting and understanding these models. The challenge is not just fitting the model; it is also deciding what models to fit.</p>
<p>Future work, by ourselves and others, will increase the speed and scalability of Stan in various ways, including more seamless implantation of parallel processing.</p>
</div>
<div id="appealing-features-of-bayesian-inference" class="section level3 unnumbered">
<h3>Appealing features of Bayesian inference</h3>
<p>Here are some reasons we like to use Bayesian methods:</p>
<ul>
<li><p>Integration of data and prior information</p></li>
<li><p>Quantification of uncertainty, including probabilistic predictions</p></li>
<li><p>Ability to pipe inference directly into decision analysis</p></li>
<li><p>Ability to handle uncertainty in large numbers of parameters</p></li>
</ul>
<p>It is said that the most important aspect of a statistical analysis is not what you do with the data, it’s what data you use. A key advantage of modern statistical methods (including Bayesian methods but also various non-Bayesian or semi-Bayesian approaches in machine learning) is that they allow you to incorporate different sorts of information into your analysis.</p>
</div>
<div id="some-things-that-bayesian-inference-and-stan-cant-do" class="section level3 unnumbered">
<h3>Some things that Bayesian inference and Stan can’t do</h3>
<p>Bayesian inference does not solve all statistical problems, though. One important class of problems where it is not currently possible to perform fully Bayesian inference is nonlinear classification and optimization with large datasets: familiar examples include language processing, speech and image recognition, and those computer programs that play Go or ping-pong. These problems are often attacked using Bayesian models, but the inferences used are typically only rough approximations to the mathematical Bayesian posterior distribution: the required calculations are simply too involved, and the posterior distributions tend to be multimodal and essentially impossible to fully navigate using any existing algorithm. Stan is not the best tool for these problems. We do think, however, that Stan is the best tool for fitting continuous-parameter models that arise in many application areas, including astronomy, ecology, economic forecasting, earth science, insurance, public health, survey sampling, to just name a few. A wide-ranging set of case studies is available on the Stan website at: <a href="http://mc-stan.org/users/documentation/case-studies" class="uri">http://mc-stan.org/users/documentation/case-studies</a> and in the conference proceedings from every StanCon: <a href="https://github.com/stan-dev/stancon_talks" class="uri">https://github.com/stan-dev/stancon_talks</a>.</p>
</div>
</div>
<div id="hello-world" class="section level2">
<h2><span class="header-section-number">1.2</span> Hello World</h2>
<p><em>Bayesian inference</em> is a framework for estimating parameters and constructing predictions given probability models and data. <em>Bayesian data analysis</em> is the larger process of building, fitting, and checking probability models. <em>Stan</em> is an open-source computer program for Bayesian inference and simulation. Stan can be run from R, Python, Julia, or other scientific/statistical software. In the examples in this book, we set up data and run Stan from R, but our focus is on Stan, not the R code.</p>
<div id="getting-started" class="section level3 unnumbered">
<h3>Getting started</h3>
<p>Go to the Stan webpage (<a href="http://mc-stan.org" class="uri">http://mc-stan.org</a>) and navigate to users and interfaces. Instructions for setting up Stan for use within R are here: <a href="http://mc-stan.org/users/interfaces/rstan.html" class="uri">http://mc-stan.org/users/interfaces/rstan.html</a>. Follow all the steps on that page.</p>
</div>
<div id="a-stan-program-for-simple-linear-regression" class="section level3 unnumbered">
<h3>A Stan program for simple linear regression</h3>
<p>Our “Hello World” example is a linear regression, <span class="math display">\[y_i=a+bx_i +\mbox{error}_i, \mbox{ for } i=1,\dots,N,\]</span> with errors independent and normally distributed with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Just one little thing to be careful of: in statistical notation, the normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> is written as <span class="math inline">\(N(\mu,\sigma^2)\)</span>. In Stan, we write it as <code>normal(mu, sigma)</code> (<em>not</em> <code>sigma^2</code>). Also, note that when we are talking about the elements of a Stan program we use <code>typewriter font</code> and when we are talking about the elements of a statistical model we use Greek letters and math notation.</p>
<p>Here is the full model written in Stan:</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real a;
  real b;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(a + b * x, sigma);
}</code></pre>
<p>The program has three named blocks: the data block which declares the inputs to the model, the parameters block which declares the parameters to be estimated, and the model block which presents the statistical model. In this program the variables <code>y</code> and <code>x</code> are vectors of length <code>N</code> which come as input data and the variables <code>a</code>, <code>b</code>, and <code>sigma</code> are scalar values which are estimated jointly by the model.</p>
<p>In order to run the program, you must provide inputs for all variables declared in the data block. Therefore we need to simulate some data for the vector length <code>N</code> and a vector of values for both <code>x</code> and <code>y</code> using known values for model parameters <code>a</code>, <code>b</code>, and <code>sigma</code>. We can do this using Stan or directly in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">100</span>
a &lt;-<span class="st"> </span><span class="dv">10</span>
b &lt;-<span class="st"> </span><span class="dv">4</span>
sigma &lt;-<span class="st"> </span><span class="dv">5</span>
x &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dv">0</span>, <span class="dv">10</span>)
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, a <span class="op">+</span><span class="st"> </span>b<span class="op">*</span>x, sigma)
hello_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N=</span>N, <span class="dt">x=</span>x, <span class="dt">y=</span>y)</code></pre></div>
<p>The last statement creates a single R data structure <code>hello_data</code> which contains the names and values of all inputs. This object is passed into the Stan algorithm which fits the the model with the simulated data. In this example we use Stan’s default sampling algorithm:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="st">&quot;stan/simplest-regression.stan&quot;</span>, <span class="dt">data=</span>hello_data)</code></pre></div>
<p>To evaluate the fit, we can print the estimates of the model parameters <code>a</code>, <code>b</code>, and <code>sigma</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;b&quot;</span>,<span class="st">&quot;sigma&quot;</span>), <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))
Inference <span class="cf">for</span> Stan model<span class="op">:</span><span class="st"> </span>simplest<span class="op">-</span>regression.
<span class="dv">4</span> chains, each with iter=<span class="dv">2000</span>; warmup=<span class="dv">1000</span>; thin=<span class="dv">1</span>; 
post<span class="op">-</span>warmup draws per chain=<span class="dv">1000</span>, total post<span class="op">-</span>warmup draws=<span class="dv">4000</span>.

      mean se_mean   sd <span class="fl">2.5</span><span class="op">%  98%</span><span class="st"> </span>n_eff Rhat
a     <span class="fl">11.4</span>    <span class="fl">0.02</span> <span class="fl">0.97</span>  <span class="fl">9.4</span> <span class="fl">13.2</span>  <span class="dv">1955</span>    <span class="dv">1</span>
b      <span class="fl">3.8</span>    <span class="fl">0.00</span> <span class="fl">0.16</span>  <span class="fl">3.5</span>  <span class="fl">4.1</span>  <span class="dv">1921</span>    <span class="dv">1</span>
sigma  <span class="fl">4.9</span>    <span class="fl">0.01</span> <span class="fl">0.36</span>  <span class="fl">4.3</span>  <span class="fl">5.7</span>  <span class="dv">2275</span>    <span class="dv">1</span>

Samples were drawn using <span class="kw">NUTS</span>(diag_e) at Wed Feb  <span class="dv">6</span> <span class="dv">21</span><span class="op">:</span><span class="dv">44</span><span class="op">:</span><span class="dv">39</span> <span class="dv">2019</span>.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split <span class="kw">chains</span> (at 
convergence, <span class="dt">Rhat=</span><span class="dv">1</span>).</code></pre></div>
<p>To simulate the data <code>y</code> we picked <code>10</code>, <code>4</code> and <code>5</code> as the values of <code>a</code>, <code>b</code>, and <code>sigma</code>, respectively. As the true values are within the inferential ranges, we have reason to trust our computation. In the <a href="workflow-in-action.html#workflow-in-action">workflow chapter</a> we show more ways to assess parameter recovery.</p>
</div>
</div>
<div id="bayesian-workflow-for-model-checking-and-model-improvement" class="section level2 unnumbered">
<h2>Bayesian workflow for model checking and model improvement</h2>
<p>In Bayesian inference we make a sort of deal with the devil: we commit to a strong model, and from this we get strong inferences. But, as the saying goes, with great power comes great responsibility. We need to vigilantly <em>check</em> the fit of our models, following this up with model <em>improvement</em>. As a result, Bayesian workflow does not involve fitting just one model to data. We typically fit multiple models, including some models that we know are too simple (to get a sense of what is lost by not including certain features in our analysis) and others that we suspect are too complex (to get a sense of the boundaries of what we can learn given the resolution of the our available data).</p>
<p><em>Model checking</em> consists of the following steps:</p>
<ol style="list-style-type: decimal">
<li><p><em>Simulate fake data.</em> Specify sizes and values for all predictors in a model and choose a set of values for all model parameters and generate the corresponding data values <span class="math inline">\(y\)</span>.</p></li>
<li><p><em>Fit the model.</em> Express the model in Stan, pass the simulated data into the program, and estimate the parameters.</p></li>
<li><p><em>Evaluate the fit.</em> Compare the estimated parameters (or, more fully, the posterior distribution of the parameters) to their true values, which in this simulated-data scenario are known.</p></li>
</ol>
<p>These are exactly the steps that we just carried out in our “Hello World” example in the previous section.</p>
<p><em>Model improvement</em> follows the idealized steps listed on the first page of <em>Bayesian Data Analysis</em>:</p>
<ol style="list-style-type: decimal">
<li><p><em>Set up a full probability model.</em> Define a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.</p></li>
<li><p><em>Condition on observed data.</em> Calculate and interpret the appropriate posterior distribution – the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.</p></li>
<li><p><em>Evaluate the fit of the model and the implications of the resulting posterior distribution.</em> How well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions? In response, one can alter or expand the model and repeat.</p></li>
</ol>
<p>This workflow helps us to think generatively about the underlying scientific problem expressed by the model. While our models are imperfect compared to the true data generating process, the above procedure provides a clear specification of the data generating process encoded by our model.</p>
<div id="idealized-plan-for-bayesian-case-studies" class="section level3 unnumbered">
<h3>Idealized plan for Bayesian case studies</h3>
<p>The <a href="workflow-in-action.html#workflow-in-action">workflow chapter</a> of this book includes several case studies to give some sense of Bayesian modeling on some fairly simple problems. Many more case studies are available on the Stan website at: <a href="http://mc-stan.org/users/documentation/case-studies" class="uri">http://mc-stan.org/users/documentation/case-studies</a> and in the StanCon conference proceedings at: <a href="https://github.com/stan-dev/stancon_talks" class="uri">https://github.com/stan-dev/stancon_talks</a>.</p>
<p>Presentation of these examples vary, but the paradigmatic format for a case study would follow these steps:</p>
<ol style="list-style-type: decimal">
<li><p>Applied example to give context</p></li>
<li><p>Discussion of reasonable parameter values and fake-data simulation in Stan or in your scientific computing environment of choice</p></li>
<li><p>Graph of fake data</p></li>
<li><p>Stan program</p></li>
<li><p>Fit fake data in Stan; discuss convergence, model diagnostics, and parameter estimates and uncertainties</p></li>
<li><p>Graph the fitted model along with the data</p></li>
<li><p>Fit real data in Stan</p></li>
<li><p>Graph the fit</p></li>
<li><p>Model checking</p></li>
<li><p>Directions for model expansion</p></li>
</ol>
</div>
</div>
<div id="writing-a-stan-program" class="section level2 unnumbered">
<h2>Writing a Stan program</h2>
<p>When you write a Stan program, you’re writing C++ code that gives instructions for computing an “objective function.” In this book we will be using Stan for Bayesian inference, and the objective function is interpreted as the logarithm of the posterior density, up to an arbitrary constant.</p>
<p>A Stan program includes various blocks to declare data and parameters and make transformations, but the heart of a Stan program, where it computes the objective function, is in the model block. The Stan program above has the following model block:</p>
<pre><code>model {
  y ~ normal(a + b * x, sigma);
}</code></pre>
<p>In this case, <code>y</code> and <code>x</code> are vectors of length <code>N</code> that are input to the Stan program as data and <code>a</code>, <code>b</code>, and <code>sigma</code> are the parameters estimated jointly by the model. The above code is mathematically (but not computationally) equivalent to:</p>
<pre><code>model {
  for (n in 1:N) {
    y[n] ~ normal(a + b * x[n], sigma);
  }
}</code></pre>
<p>Each line inside the loop adds a term to the objective function with the logarithm of the corresponding normal density; thus, <span class="math display">\[\log(\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2}(\frac{y_n - (a +
bx_n)}{\sigma})^2)) = - \frac{1}{2}\log(2\pi) - \frac{1}{2}\log\sigma
- \frac{1}{2}(\frac{y_n - (a + bx_n)}{\sigma})^2\]</span> For most purposes, we do not care about arbitrary multiplicative constants in the posterior density or, equivalently, arbitrary additive constants in the log-posterior density, so it does not matter if the <span class="math inline">\(-\frac{1}{2}\log(2\pi)\)</span> term is present. We <em>do</em> need to include <span class="math inline">\(-\frac{1}{2}\log\sigma\)</span>, however, because <span class="math inline">\(\sigma\)</span> is a parameter in the model and thus we cannot consider this term as constant.</p>
<p>For reasons that we shall discuss later, the above code is more efficient in vectorized form (without the loop).</p>
<p>The relevant point of the above discussion is that the model block is where the objective function is computing, with distributional statements corresponding to increments to the objective function. We can make this explicit by rewriting the above model block as:</p>
<pre><code>model {
  target += normal_lpdf(y | a + b * x, sigma);
}</code></pre>
<p>or</p>
<pre><code>model {
  for (n in 1:N) {
    target += y[n] ~ normal_lpdf(y[n] | a + b * x[n], sigma);
  }
}</code></pre>
<p>Here, “target” is the objective function, “lpdf” stands for “log probability density function,” and the vertical bar is statistics notation for conditioning: thus, we are adding to the objective function the normal log density function of <code>y</code>, given mean <code>a+bx</code> and standard deviation <code>sigma</code>.</p>
<p>Here is a slightly more elaborate version, in which we include <span class="math inline">\(\mbox{normal}(0,1)\)</span> prior distributions for <code>a</code>, <code>b</code>, and <code>sigma</code> (actually the prior for <code>sigma</code> is half-normal as this parameter has been constrained to be positive). We will talk more about priors in a bit; for here, you can just think of these extra statements as representing additional information that the parameters <code>a</code>, <code>b</code>, and <code>sigma</code> are likely to be not too far from 0:</p>
<pre><code>model {
  y ~ normal(a + b * x, sigma);
  a ~ normal(0, 1); b ~ normal(0, 1);
  sigma ~ normal(0, 1);
}</code></pre>
<p>Or, equivalently:</p>
<pre><code>model {
  target += normal_lpdf(y | a + b * x, sigma);
  target += normal_lpdf(a | 0, 1);
  target += normal_lpdf(b | 0, 1);
  target += normal_lpdf(sigma | 0, 1);
}</code></pre>
<p>Every line in the model block with a tilde (~) corresponds to an augmentation of the target, or objective function.</p>
<p>We can also include lines in the code that do <em>not</em> augment the objective function. For example:</p>
<pre><code>model {
  // expected value of y when x=2 y ~ normal(a + b * x, sigma);
  real a_shifted = a + 2 * b;
  a_shifted ~ normal(0, 1);
  b ~ normal(0, 1);
  sigma ~ normal(0, 1);
}</code></pre>
<p>Here we wanted to assign a prior distribution not to the parameter <code>a</code> but to the shifted parameter <code>a+2b</code>. The above code is executed directly, with an augmentation of “target” for every line with a tilde.</p>
<div id="interpreting-the-objective-function-as-the-log-posterior-density" class="section level3 unnumbered">
<h3>Interpreting the objective function as the log posterior density</h3>
<p>That is, the objective function is interpreted as being the log posterior density of the parameters, plus some arbitrary constant. Mathematically, if the objective function computed by Stan is <span class="math inline">\(g(\theta,y)\)</span>, then the implied posterior density is <span class="math inline">\(p(\theta|y)=\frac{1}{Z(y)}e^{g(\theta,y)}\)</span>, where <span class="math inline">\(Z(y)=\int g(\theta,y)d\theta\)</span>. To perform inference using this posterior distribution, is not necessary to actually evaluate the integral <span class="math inline">\(Z\)</span>; it is enough to be able to compute <span class="math inline">\(g\)</span>.</p>
</div>
<div id="stans-fitting-algorithms" class="section level3 unnumbered">
<h3>Stan’s fitting algorithms</h3>
<p>As described above, a Stan program can be viewed as instructions for computing an objective function. When you run a Stan program, it performs optimization or approximation or sampling of this objective function, using one of the algorithms described below.</p>
<div id="option-1-sampling" class="section level4 unnumbered">
<h4>Option 1: Sampling</h4>
<p>Usually when we run Stan we use it to <em>sample</em> from the posterior distribution that is proportional to the exponential of the objective function. Currently available in Stan are two different sampling algorithms: Hamiltonian Monte Carlo (HMC) and the no-U-turn sampler (NUTS). For background on HMC, see Chapter 12 of <em>Bayesian Data Analysis</em>. NUTS is an adaptive version of HMC and is the algorithm that we use by default when fitting models in Stan. For more information on NUTS see Chapter “MCMC Sampling” of the <em>Stan Reference Manual</em>. When running NUTS (or HMC, or any other sampling algorithm), the output of Stan is a list of posterior simulations.</p>
<p>We have already seen an example of this above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="st">&quot;stan/simplest-regression.stan&quot;</span>, <span class="dt">data=</span>hello_data)</code></pre></div>
<p>The output of the Stan run, saved in R as the object “fit”, contains posterior simulations and also some other information regarding the settings of the fitting algorithm. Here, for example, we access some simulations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sims_as_matrix &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit)
<span class="kw">print</span>(sims_as_matrix[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,])
          parameters
iterations  a   b sigma lp__
      [<span class="dv">1</span>,] <span class="dv">12</span> <span class="fl">3.7</span>   <span class="fl">5.1</span> <span class="op">-</span><span class="dv">207</span>
      [<span class="dv">2</span>,] <span class="dv">12</span> <span class="fl">3.8</span>   <span class="fl">4.8</span> <span class="op">-</span><span class="dv">206</span>
      [<span class="dv">3</span>,] <span class="dv">12</span> <span class="fl">3.9</span>   <span class="fl">4.8</span> <span class="op">-</span><span class="dv">207</span>
      [<span class="dv">4</span>,] <span class="dv">11</span> <span class="fl">3.8</span>   <span class="fl">4.6</span> <span class="op">-</span><span class="dv">206</span>
      [<span class="dv">5</span>,] <span class="dv">12</span> <span class="fl">3.8</span>   <span class="fl">4.8</span> <span class="op">-</span><span class="dv">206</span>
sims_as_list &lt;-<span class="st"> </span><span class="kw">extract</span>(fit)
<span class="kw">print</span>(sims_as_list<span class="op">$</span>a[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>])
[<span class="dv">1</span>] <span class="dv">11</span> <span class="dv">12</span> <span class="dv">13</span> <span class="dv">12</span> <span class="dv">12</span></code></pre></div>
<p>Each row of the above matrix is a different posterior simulation: we see the parameters <code>a</code>, <code>b</code>, and <code>sigma</code>, along with the objective function (or log-posterior density), “lp__“.</p>
</div>
<div id="option-2-optimization" class="section level4 unnumbered">
<h4>Option 2: Optimization</h4>
<p>The simplest way to run Stan is on optimize setting; it then uses the BFGS algorithm to find (or attempt to find) the maximum of the objective function, and Stan returns the value of the parameters at this estimated mode along with the computed value of the objective function. Here is an example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">simplest &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;stan/simplest-regression.stan&quot;</span>)
fit_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">optimizing</span>(simplest, <span class="dt">data=</span>hello_data)</code></pre></div>
<p>In this case we broke the computation into two steps, first compiling and saving the compiled model into the R object “simplest” and then performing the optimizer on this program. And here is the result:</p>
<pre><code>$par
    a     b sigma 
 11.4   3.8   4.8 

$value
[1] -207

$return_code
[1] 0</code></pre>
<p>For this simple model, the values of the parameters at the optimum are close to the posterior median estimates obtained from to the NUTS fit earlier. We also get the value of the objective function, which has no direct interpretation in this case, and a return code which represents the stability of the optimizer. (A return code of 0 is good news.)</p>
<p>Interpreting the objective function as the log posterior density, the result of the optimizer is the posterior mode. If the model has a uniform prior distribution, then this is also the maximum likelihood estimate.</p>
<p>In classical statistics, the maximum likelihood is accompanied by an uncertainty estimate based on the curvature of the log-likelihood function; see Sections 4.1 and 13.3 of <em>Bayesian Data Analysis</em> for a corresponding Bayesian interpretation based on the posterior mode. There are times when this mode-based approximation makes sense—notably when datasets are so large that full Bayesian computation is too slow to be practical—but usually we fit our Stan models using simulation, so that we get posterior uncertainties directly, with no need for any normal approximation.</p>
</div>
<div id="option-3-distributional-approximations" class="section level4 unnumbered">
<h4>Option 3: Distributional approximations</h4>
<p>Stan also allows you to fit models using ADVI (autodifferentiated variational inference). As an approximation to the posterior distribution, ADVI can work better than simple mode-based approximations and it can be much faster for large problems, but currently the way ADVI is implemented in Stan, there are concerns about its convergence. We have applied ADVI to a few hundred small example models where we can compare to the full posterior distribution: in many of these examples, ADVI works well, but in some examples, ADVI gives apparent convergence but to the wrong answer that is not a good approximation to the posterior. Hence, when using ADVI, it is important to do some fake-data simulation to check that the algorithm can recover something close to the underlying model. For now, it is probably best to think of ADVI as an experimental tool that you should only try for problems that are too large to solve using regular NUTS.</p>
</div>
</div>
<div id="iterations-and-steps" class="section level3 unnumbered">
<h3>Iterations and steps</h3>
<p>Stan’s fitting algorithms are iterative. The algorithm starts with some initial values for all the parameters (these can be user-specified and fed into the call to Stan, or else Stan will simulate them from a default distribution; (see Chapter “MCMC Sampling”, section “Initialization” of the <em>Stan Reference Manual</em>) and then, at each <em>iteration</em>, the parameter values are updated. After enough iterations, the parameter values converge to an estimate of the optimum (if Stan is being set to optimization) or to an estimate of the center of the distribution (if Stan is being set to distributional approximation) or the iterations will wander through the posterior distribution (if Stan is being set to sample).</p>
<p>In the HMC and NUTS algorithms (and recall that NUTS is Stan’s default), each iteration includes some number of <em>steps</em>. In each step, Stan computes the gradient of the objective function, which is used in the “leapfrog algoritm” to determine the direction in which the algorithm moves (see Section 12.4 of <em>Bayesian Data Analysis</em> and the “MCMC Sampling” chapter of the <em>Stan Reference Manual</em>). An iteration might have tens or hundreds of steps. The information from each step is not saved by Stan; what is returned is the value of the parameters at the end of each iteration.</p>
<p>As a user, you typically don’t need to worry about steps or iterations; all that is relevant is that Stan returns posterior simulation draws of the parameters. But when Stan has convergence problems or when it takes a long time to fit a model, then it can be helpful to understand what is happening under the hood, as often it can be possible to reparameterize or reprogram the model to run more efficiently.</p>
</div>
<div id="adaptation-and-warmup" class="section level3 unnumbered">
<h3>Adaptation and warmup</h3>
<p>The iterative algorithms have <em>tuning parameters</em>. These are not statistical parameters in the model to be estimated; rather, they are settings that need to be adjusted for the algorithms to run efficiently. The sampling algorithms in Stan have <em>warmup</em> and <em>production</em> stages. In the warmup stage, the tuning parameters are altered using various heuristics with the goal of efficient movement during the production stages, whose iterations are saved. In the current default settings, Stan runs for 1000 warmup iterations and 1000 production iterations, but we would like to change these defaults to become more adaptive, running fewer iterations if that is all that is needed, and automatically running more when 2000 iterations are not enough.</p>
</div>
<div id="multiple-chains-mixing-r-hat-effective-sample-size" class="section level3 unnumbered">
<h3>Multiple chains, mixing, R-hat, effective sample size</h3>
<p>To have trust in the results of an iterative algorithm, it can be helpful to run it from different starting points and check that the different runs of the algorithm reach the same endpoint. Each run of the algorithm is called a “chain” because the algorithms can often be described mathematically as Markov chains; hence we speak of <em>multiple chains</em>. By default, when run from R, Stan simulates 4 chains in parallel, which is convenient on many laptop computers which have 4 processors.</p>
<p>For optimization or distributional approximation, we literally want the different chains to converge to the same spot. For sampling, we want the different chains to trace out the same distribution, which we measure by comparing the simulations from different chains to each other. For each parameter or quantity of interest saved, Stan reports “R-hat,” a numerical measure which is larger than 1 when chains have not mixed well and becomes close to 1 when the chains have mixed. We typically run Stan for the default number of iterations and, if R-hat is less than 1.1 for all parameters, we just use the simulations to represent the posterior distribution. If R-hat is greater than 1.1 for any parameters, the algorithm is slow to converge and we might run it longer and then, if mixing remains poor, consider reparameterization of the problem.</p>
<p>Stan also computes for each parameter, transformed parameter, and generated quantity an “effective sample size” which represents the number of uncorrelated draws in the sample. For more information see section “Effective Sample Size” in Chapter “Posterior Analysis” of the <em>Stan Reference Manual</em>.</p>
</div>
<div id="blocks-and-declarations" class="section level3 unnumbered">
<h3>Blocks and declarations</h3>
<p>As the language is currently configured, a Stan program is divided into <em>blocks</em>:</p>
<ul>
<li><p>Data. Here you declare all the information that must be supplied for the program to run. For example, in a regression model, the data would be x, y, and N. Each data object in a Stan program must be given a type, such as int (integer), real, vector, matrix, etc. Explanations of all the types are in chapter on <a href="#matrices-vectors-and-arrays">matrices, vectors, and arrays</a> of this book.</p></li>
<li><p>Transformed data. These are mathematical operations that are performed once when the Stan program is called.</p></li>
<li><p>Parameters. These are the unknown quantities that are estimated when the Stan program is run. Parameters also need to be declared, and they need to be continuous. You cannot have an integer-valued parameter. See the <a href="#latent-discrete.chapter">latent discrete parameters</a> chapter for further discussion of this point.</p></li>
<li><p>Transformed parameters. Computations that depend on parameters as well as data, thus must be computed in each step when Stan runs.</p></li>
<li><p>Model. This is the part of the Stan program where the objective function is computed. As discussed above, at each step, Stan computes the objective function by starting at zero and then adding to it with each statement that includes “target +=” or “~”.</p></li>
<li><p>Generated quantities. These are computed at the end of each iteration (not each step) and are saved.</p></li>
</ul>
</div>
<div id="vectors-and-arrays" class="section level3 unnumbered">
<h3>Vectors and arrays</h3>
<p>Here is an example of a Stan model with a vector parameter:</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  int&lt;lower=0&gt; K;
  matrix[N,K] X;
  vector[N] y;
}
parameters {
  vector[K] b;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(X*b, sigma);
}</code></pre>
<p>To fit this model, we simply reconfigure the data by constructing the matrix of predictors and then run from R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ones &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N)
X &lt;-<span class="st"> </span><span class="kw">cbind</span>(ones, x)
K &lt;-<span class="st"> </span><span class="dv">2</span>
data_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N=</span>N, <span class="dt">K=</span>K, <span class="dt">X=</span>X, <span class="dt">y=</span>y)
fit_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="st">&quot;stan/vector-regression.stan&quot;</span>, <span class="dt">data=</span>data_<span class="dv">3</span>)</code></pre></div>
<p>Here is the result:</p>
<pre><code>Inference for Stan model: vector-regression.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

        mean se_mean   sd   2.5%    25%    50%    75%    98% n_eff Rhat
b[1]    11.4    0.02 0.99    9.4   10.7   11.3   12.0   13.3  1635    1
b[2]     3.8    0.00 0.16    3.5    3.7    3.8    3.9    4.1  1660    1
sigma    4.9    0.01 0.35    4.3    4.7    4.9    5.2    5.7  2507    1
lp__  -207.0    0.04 1.23 -210.2 -207.6 -206.7 -206.1 -205.6  1237    1

Samples were drawn using NUTS(diag_e) at Wed Feb  6 21:45:30 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>And here is what happens if we grab the matrix or list of posterior simulations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sims_as_matrix &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit_<span class="dv">3</span>)
<span class="kw">print</span>(sims_as_matrix[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,])
          parameters
iterations b[<span class="dv">1</span>] b[<span class="dv">2</span>] sigma lp__
      [<span class="dv">1</span>,]   <span class="dv">12</span>  <span class="fl">3.7</span>   <span class="fl">4.5</span> <span class="op">-</span><span class="dv">206</span>
      [<span class="dv">2</span>,]   <span class="dv">11</span>  <span class="fl">3.8</span>   <span class="fl">4.8</span> <span class="op">-</span><span class="dv">206</span>
      [<span class="dv">3</span>,]   <span class="dv">11</span>  <span class="fl">3.9</span>   <span class="fl">5.1</span> <span class="op">-</span><span class="dv">206</span>
      [<span class="dv">4</span>,]   <span class="dv">10</span>  <span class="fl">3.9</span>   <span class="fl">5.2</span> <span class="op">-</span><span class="dv">207</span>
      [<span class="dv">5</span>,]   <span class="dv">12</span>  <span class="fl">3.8</span>   <span class="fl">5.2</span> <span class="op">-</span><span class="dv">206</span>
sims_as_list &lt;-<span class="st"> </span><span class="kw">extract</span>(fit_<span class="dv">3</span>)
<span class="kw">print</span>(sims_as_list<span class="op">$</span>b[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,])
          
iterations [,<span class="dv">1</span>] [,<span class="dv">2</span>]
      [<span class="dv">1</span>,]   <span class="dv">12</span>  <span class="fl">3.6</span>
      [<span class="dv">2</span>,]   <span class="dv">11</span>  <span class="fl">3.8</span>
      [<span class="dv">3</span>,]   <span class="dv">12</span>  <span class="fl">3.8</span>
      [<span class="dv">4</span>,]   <span class="dv">11</span>  <span class="fl">3.9</span>
      [<span class="dv">5</span>,]   <span class="dv">11</span>  <span class="fl">3.7</span></code></pre></div>
<p>The parameter <code>b</code> is a vector, and so the posterior simulations are a 2-dimensional array. Similarly, if <code>b</code> is a matrix or array of vectors, the posterior simulations will become a 3-dimensional array, and so on. We shall encounter such parameter structures when fitting hierarchical models.</p>
</div>
</div>
<div id="software-environment" class="section level2">
<h2><span class="header-section-number">1.3</span> Software environment</h2>
<div id="working-in-r-and-stan" class="section level3 unnumbered">
<h3>Working in R and Stan</h3>
<p>In this book, we will work with data and fit Stan models using the Rstan package in R, as demonstrated above. Our focus, though, is on statistical modeling and writing Stan programs, so in most of the book we suppress the R. All the code is available (see below) so that you can reproduce any of the computations we perform.</p>
</div>
<div id="other-stan-interfaces-python-julia-command-line-and-more" class="section level3 unnumbered">
<h3>Other Stan interfaces: Python, Julia, command line, and more</h3>
<p>Stan can also be run from the command line or from other statistical environments such as Python and Julia. The Stan website contains a list of interfaces and installation instructions at <a href="http://mc-stan.org/users/interfaces/index.html" class="uri">http://mc-stan.org/users/interfaces/index.html</a>.</p>
</div>
<div id="source-code-for-this-book-and-its-programs-scripts-data" class="section level3 unnumbered">
<h3>Source code for this book and its programs, scripts, data</h3>
<p>You may be reading this book as an html or pdf file or even as a printed document. The underlying content lives in the GitHub repository <a href="https://github.com/stan-dev/docs" class="uri">https://github.com/stan-dev/docs</a> in the subdirectory <a href="https://github.com/stan-dev/docs/tree/master/src/bayes-stats-stan" class="uri">https://github.com/stan-dev/docs/tree/master/src/bayes-stats-stan</a>. Each chapter of the book is a “knitr file,” which means that it can be run in R, most conveniently using Rstudio. You can also go to the raw knitr docs (again, most conveniently using the editor in Rstudio): these are the files with .Rmd extensions. These documents have the text of the book (e.g., the words you are reading now) along with R code for analysis and visualization, some of which is hidden in this html or pdf document that you are reading but is visible in the raw .Rmd documents.</p>
<p>The Stan programs used in this book are all in <code>.stan</code> files in the directory <a href="https://github.com/stan-dev/docs/tree/master/src/bayes-stats-stan/stan" class="uri">https://github.com/stan-dev/docs/tree/master/src/bayes-stats-stan/stan</a>.</p>
</div>
</div>
<div id="bayes-and-stan" class="section level2">
<h2><span class="header-section-number">1.4</span> Bayes and Stan</h2>
<p>Bayesian inference is a general approach for learning about unknown parameters and making predictions about missing or latent data, given observed data and a probability model. For the theory and practice of Bayesian inference, see our book <em>Bayesian Data Analysis</em>. Or, for a more introductory treatment, the book <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em> by Richard McElreath. We recommend you read those books along with this one.</p>
<p>Stan is a computer program that performs Bayesian inference on any model with continuous parameters whose log-posterior density can be computed (up to an arbitrary additive constant, as discussed above).</p>
<p>If you want to fit a Bayesian model <em>not</em> using Stan, you can perform computations analytically or by direct simulation for some simple models (see the early chapters of <em>Bayesian Data Analysis</em> for many examples), or write your own customized simulation algorithm (see Part III of <em>Bayesian Data Analysis</em>), or use some other general-purpose software. We like Stan, both for fitting Bayesian models in our applied work, and for teaching the general principles of Bayesian modeling.</p>
<div id="what-cant-be-fit-in-stan" class="section level3 unnumbered">
<h3>What <em>can’t</em> be fit in Stan?</h3>
<p>There are several sorts of models that can’t be fit, or can’t be fit well, using Stan. Other software has problems with such models too.</p>
<ul>
<li>Models with discrete parameters. Stan can only fit continuous-parameter models. If you have a model with discrete parameters, you have to fit it using some other approach. Very simple discrete-parameter models can be fit using direct computation of the posterior distribution; we give two such examples in Section 1.4 of <em>Bayesian Data Analysis</em>. For more complicated examples, we can sometimes average over the discrete parameters analytically; see the <a href="#mixture-modeling.chapter">mixture modeling</a> chapter of this book for general discussion and examples of finite mixture and hidden Markov models. More generally, posterior distributions for models with discrete unknowns can be explored using various stochastic algorithms, but except in certain special cases these can be very slow to converge: the challenge is to perform a tour of a discrete space without simply going through all the possible values, which would result in a combinatorial explosion. Thus, we sometimes say that Stan can’t walk through discrete spaces, but no other algorithm can do so effectively either.</li>
</ul>
<p>In any case, if you want to perform Bayesian inference for a model with discrete unknowns, you need to either average over the discrete variables as described in the <a href="#latent-discrete.chapter">latent discrete parameters</a> chapter, or fit the model outside of Stan. Either way, we recommend fake-data simulation to check that the fit can recover the parameter values under reasonable assumptions.</p>
<ul>
<li><p>Models with likelihoods or priors that contain uncomputable normalizing constants. This comes up sometimes with spatial or network models. We won’t discuss these further here; you can see Section 13.10 of <em>Bayesian Data Analysis</em> for a brief general discussion of the problem.</p></li>
<li><p>Models for which the NUTS algorithm mixes very slowly. This can include continuous models with discrete aspects to their geometry, such as multimodal posterior distributions. Sometimes we can get these models to run more smoothly by reparameterizing; see the <a href="#optimization.chapter">optimization</a> chapter of this book for the important example of the use of the non-centered parameterization to avoid the “funnel problem” with hierarchical models.</p></li>
<li><p>Models for which the objective function takes a long time to compute. This includes some models with differential equations (see the <a href="#ode-solver.chapter">ordinary differential equations</a> chapter) or even simple models, if the size of the data or the number of parameters is large enough. There are various approaches to managing large and slow computations in Stan, including data subsetting, and posterior approximations, and parallel computing. For most of this book we focus on small and moderate-sized problems; we discuss big-data strategies in the <a href="#map-reduce.chapter">map-reduce</a> chapter.</p></li>
</ul>
</div>
</div>
<div id="more-information-about-bayesian-data-analysis-and-stan" class="section level2">
<h2><span class="header-section-number">1.5</span> More information about Bayesian data analysis and Stan</h2>
<p>Here is a list of resources, roughly in order from most to least formal:</p>
<ul>
<li><p>The Stan Reference Manual: <a href="http://mc-stan.org/users/documentation/index.html" class="uri">http://mc-stan.org/users/documentation/index.html</a> which has details on the algorithms and implementations used by Stan, along with all of Stan’s functions.</p></li>
<li><p><em>Bayesian Data Analysis</em> by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (third edition, 2014)</p></li>
<li><p><em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em> by McElreath (2015)</p></li>
<li><p><em>Regression and Other Stories</em> by Gelman, Hill, and Vehtari (forthcoming)</p></li>
<li><p>Stan case studies: <a href="http://mc-stan.org/users/documentation/case-studies.html" class="uri">http://mc-stan.org/users/documentation/case-studies.html</a> and <a href="https://github.com/stan-dev/stancon_talks" class="uri">https://github.com/stan-dev/stancon_talks</a></p></li>
<li><p>Stan Discourse list: <a href="https://discourse.mc-stan.org" class="uri">https://discourse.mc-stan.org</a></p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="part-1-bayesian-workflow.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fake-data-simulation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
