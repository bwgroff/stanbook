<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan</title>
  <meta name="description" content="3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan, with examples and programming techniques.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan, with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan" />
  
  <meta name="twitter:description" content="3 Prior Distributions and Models for Data | Bayesian Workflow Using Stan, with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="fake-data-simulation.html">
<link rel="next" href="some-self-contained-examples.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="part-2-bayesian-inference.html#part-2-bayesian-inference"><i style="font-size: 110%; color:#990017;">Part 2: Bayesian Inference</i></a></li>
<li class="chapter" data-level="7" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>7</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="8" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>8</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="9" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="10" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>10</b> Variational Inference</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Workflow Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prior-distributions-and-models-for-data" class="section level1">
<h1><span class="header-section-number">3</span> Prior Distributions and Models for Data</h1>
<div id="prior-likelihood-posterior-densities-bayes-as-information-aggregation" class="section level2">
<h2><span class="header-section-number">3.1</span> Prior, likelihood, posterior densities; Bayes as information aggregation</h2>
<p>As discussed in the context of the Hello World example in chapter 1, a Stan model is constructed by adding to the objective function or log-posterior density; each line in the model block with “target +=” or a “~” symbol adds a term. The increments can come in any order, as addition is commutative (<span class="math inline">\(X+Y = Y+X\)</span>). So your Stan model can start with the log-prior and then add the log-likelihood, or the other way around. Or the two can be interspersed. The structure of Stan can be thought of as an implementation of the general idea of Bayesian inference as information aggregation: Prior information, data information, all of it is just information that is fed into the inference engine.</p>
</div>
<div id="example-apparently-duplicate-priors" class="section level2">
<h2><span class="header-section-number">3.2</span> Example: apparently duplicate priors</h2>
<p>Start with the following simple Stan program:</p>
<pre><code>data {
  real y;
}
parameters {
  real theta;
}
model {
  theta ~ normal(0, 1);
  y ~ normal(theta, 1);
}</code></pre>
<p>Here, the prior and data happen to provide the same amount of information about <span class="math inline">\(\theta\)</span>, and the posterior mean of <span class="math inline">\(\theta\)</span> will fall halfway between the data, <span class="math inline">\(y\)</span>, and the prior mean, 0.</p>
<p>Now suppose we add a new line to the model block, apparently duplicating the prior:</p>
<pre><code>model {
  theta ~ normal(0, 1);
  theta ~ normal(0, 1);
  y ~ normal(theta, 1);
}</code></pre>
<p>This looks weird. We have specified a model for <span class="math inline">\(\theta\)</span> twice. Does this cause an error in the Stan program? Does Stan simply ignore the duplicate code? Does the second “prior” overwrite the first? Actually, no. The above model block is legal Stan code, as we can see by replacing each line by its equivalent augmentation of the objective function:</p>
<pre><code>model {
  target += normal(theta | 0, 1);
  target += normal(theta | 0, 1);
  target += normal(y | theta, 1);
}</code></pre>
<p>Each line adds a piece of information to the log posterior density. In this simple example, all three lines of code contain the equivalent amount of information—each is a normal density with scale 1—and so the posterior mean of <span class="math inline">\(\theta\)</span> will be the average of the three numbers, 0, 0, and <span class="math inline">\(y\)</span>.</p>
<p>Indeed, the above code is equivalent to this:</p>
<pre><code>model {
  theta ~ normal(0, 1/sqrt(2));
  y ~ normal(theta, 1);
}</code></pre>
<p>We cannot in general do this sort of simplification but it happens that with the normal distribution these computations can be performed analytically, as the product of two normal densities is itself normal.</p>
<p>As a practical matter, we do <em>not</em> recommend code like this:</p>
<pre><code>  theta ~ normal(0, 1);
  theta ~ normal(0, 1);</code></pre>
<p>as it just looks confusing. But understanding how it works gives insight into the way that priors and data combine to express information.</p>
</div>
<div id="concentration-of-the-likelihood" class="section level2">
<h2><span class="header-section-number">3.3</span> Concentration of the likelihood</h2>
<p>For any fixed model, as the sample size increases, the likelihood becomes more informative. This can be seen, for example, in the mdoels above, where each additional data point adds another term to the likelihood in the implicit vector addition of the <code>target +=</code> or <code>~</code> statement. See Chapter 4 of <em>Bayesian Data Analysis</em> for more on this.</p>
</div>
<div id="problems-with-so-called-noninformative-priors" class="section level2">
<h2><span class="header-section-number">3.4</span> Problems with so-called noninformative priors</h2>
<p>Before discussing prior distributions more generally, we share a simple example.</p>
<p>You have a single parameter <span class="math inline">\(\theta\)</span> which we estimate using an experiment which returns an estimate <span class="math inline">\(y\)</span>. For simplicity, assume the estimate is unbiased, normally distributed, and with known standard deviation that happens to equal 1. Thus, the data model is: <span class="math display">\[
y \sim \mbox{normal}(\theta,1).
\]</span> Now suppose you perform Bayesian inference using a uniform prior on <span class="math inline">\(\theta\)</span>. Then your posterior distribution is simply (see, for example, chapter 2 of <em>Bayesian Data Analysis</em>), <span class="math display">\[
\theta|y \sim \mbox{normal}(y,1).
\]</span></p>
<p>OK, fine. Now suppose your estimate happens to be <span class="math inline">\(y=1\)</span>. There are two ways of thinking about this:</p>
<ul>
<li><p>The observed data, 1 standard error away from 0, is completely consistent with a true value of <span class="math inline">\(\theta=0\)</span>, or of low values of <span class="math inline">\(\theta\)</span> that could be either positive or negative. From our usual statistical perspective, we would say that the data are consistent with pure noise, and we cannot learn much from these data alone.</p></li>
<li><p>From the Bayesian posterior distribution, we can compute that <span class="math inline">\(\mbox{Pr}(\theta&gt;0) = 0.84\)</span> (in R, <code>pnorm(1)</code>), thus you should be willing to bet with 5-to-1 odds that <span class="math inline">\(\theta\)</span> is positive.</p></li>
</ul>
<p>This seems wrong, the idea that something recognizable as pure noise can lead to 5:1 posterior odds.</p>
<p>The problem is coming from the prior distribution. We can see this in two ways. First, just directly, effects near zero are more common than large effects.</p>
<p>Jakulin et al. (2008) argue that logistic regression coefficients are usually less than 1. So let’s try combining normal(1,1) data with a Cauchy(0,1) prior. It’s easy enough to do in Stan:</p>
<pre><code>data {
  real y;
}
parameters {
  real theta;
}
model {
  theta ~ cauchy (0, 1);
  y ~ normal(theta, 1);
}</code></pre>
<p>If we fit this model supplying data <span class="math inline">\(y=1\)</span>, and then from the resulting simulations compute the posterior probability that <span class="math inline">\(\theta &gt; 0\)</span>, we get 0.77, that is, roughly a 3:1 posterior probability that the effect is positive.</p>
<p>Just to check that we’re not missing anything, let’s re-run using the flat prior. New Stan model:</p>
<pre><code>data {
  real y;
}
parameters {
  real theta;
}
model {
  y ~ normal (theta, 1);
}</code></pre>
<p>and re-run with the same R code. This time, indeed, 84% of the posterior simulations of <span class="math inline">\(\theta\)</span> are greater than 0.</p>
<p>So far so good. Although one might argue that the posterior probability of 0.77 (from the inference given the unit Cauchy prior) is still too high. Perhaps we want a stronger prior? This sort of discussion is just fine. If you look at your posterior inference and it doesn’t make sense to you, this “doesn’t make sense” corresponds to additional prior information you haven’t included in your analysis.</p>
<p>OK, so that’s one way to consider the unreasonableness of a noninformative prior in this setting. It’s not so reasonable to believe that effects are equally likely to be any size. They’re generally more likely to be near zero.</p>
<p>The other way to see what’s going on with this example is to take that flat prior seriously. Suppose theta really could be just about anything—or, to keep things finite, suppose you wanted to assign theta a uniform prior distribution on <span class="math inline">\([-1000,1000]\)</span>, and then you gather enough data to estimate <span class="math inline">\(\theta\)</span> with a standard deviation of 1. Then, a priori, you’re nearly certain to gather very very strong information about the sign of <span class="math inline">\(\theta\)</span>. To start with, there’s a 0.998 chance that your estimate will be more than 2 standard errors away from zero so that your posterior odds about the sign of <span class="math inline">\(\theta\)</span> will be at least 20-to-1. And there’s a 0.995 chance that your estimate will be more than 5 standard errors away from zero.</p>
<p>So, in your prior distribution, this particular event—that <span class="math inline">\(y\)</span> is so close to zero that there is uncertainty about <span class="math inline">\(\theta\)</span>’s sign—is extremely unlikely. And it would be irrelevant that <span class="math inline">\(y\)</span> is not statistically significantly different from 0.</p>
<p>Modeling can be difficult. In some settings the data are strong and prior information is weak, and it’s not really worth the effort to think seriously about what external knowledge we have about the system being studied. Often, though, we do know a lot, and we’re interested in various questions where data are sparse, and we could be putting more effort into quantifying our prior distribution.</p>
<p>Upsetting situations—for example, the estimate of <span class="math inline">\(1 \pm 1\)</span> which leads to a seemingly too-strong claim of 5-to-1 odds in favor of a positive effect—are helpful in that they can reveal that we have prior information that we have not yet included in our models.</p>
</div>
<div id="general-priors.section" class="section level2">
<h2><span class="header-section-number">3.5</span> Priors</h2>
<p>The prior only matters where the likelihood is nonzero (and vice-versa). One way to think of the prior distribution is as a “regularization” or “soft constraint” on the parameters of the model. We discuss prior choice more fully on the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">prior choice wiki page</a>.</p>
<p>We consider 5 levels of priors:</p>
<ul>
<li><p>Flat prior;</p></li>
<li><p>Super-vague but proper prior: normal(0, 1e6);</p></li>
<li><p>Weakly informative prior, very weak: normal(0, 10);</p></li>
<li><p>Generic weakly informative prior: normal(0, 1);</p></li>
<li><p>Specific informative prior: normal(0.4, 0.2) or whatever. Sometimes this can be expressed as a scaling followed by a generic prior: theta = 0.4 + 0.2*z; z ~ normal(0, 1);</p></li>
</ul>
<p>In Stan, if you don’t supply any prior at all, you are implicitly using a flat prior. In general we don’t recommend this, but for many cases in this book, such as the Hello World example above, we use flat priors for simplicity and because we are focuseing on the expression of the data model or likelihood.</p>
<p>Setting up priors goes along with setting up the model. You want to set up your parameters so they are understandable, and then you will typically have some sense of their possible range, and you can use this to set up priors. If you have a lot of data, the prior won’t matter much, but there are lots of real examples where data are weak and it it is helpful to use an informative prior.</p>
<div id="background-reading" class="section level3 unnumbered">
<h3>Background Reading</h3>
<p>See <span class="citation">A. Gelman (<a href="#ref-Gelman:2006">2006</a>)</span> for an overview of choices for priors for scale parameters, <span class="citation">Chung et al. (<a href="#ref-ChungEtAl:2013">2013</a>)</span> for an overview of choices for scale priors in penalized maximum likelihood estimates, and <span class="citation">Gelman et al. (<a href="#ref-GelmanJakulinPittauEtAl:2008">2008</a>)</span> for a discussion of prior choice for regression coefficients.</p>
</div>
<div id="improper-uniform-priors" class="section level3 unnumbered">
<h3>Improper Uniform Priors</h3>
<p>The default in Stan is to provide uniform (or “flat”) priors on parameters over their legal values as determined by their declared constraints. A parameter declared without constraints is thus given a uniform prior on <span class="math inline">\((-\infty,\infty)\)</span> by default, whereas a scale parameter declared with a lower bound of zero gets an improper uniform prior on <span class="math inline">\((0,\infty)\)</span>. Both of these priors are improper in the sense that there is no way formulate a density function for them that integrates to 1 over its support.</p>
<p>Stan allows models to be formulated with improper priors, but in order for sampling or optimization to work, the data provided must ensure a proper posterior. This usually requires a minimum quantity of data, but can be useful as a starting point for inference and as a baseline for sensitivity analysis (i.e., considering the effect the prior has on the posterior).</p>
<p>Uniform priors are specific to the scale on which they are formulated. For instance, we could give a scale parameter <span class="math inline">\(\sigma &gt; 0\)</span> a uniform prior on <span class="math inline">\((0,\infty)\)</span>, <span class="math inline">\(q(\sigma) = c\)</span> (we use <span class="math inline">\(q\)</span> because the “density” is not only unnormalized, but unnormalizable), or we could work on the log scale and provide <span class="math inline">\(\log \sigma\)</span> a uniform prior on <span class="math inline">\((-\infty,\infty)\)</span>, <span class="math inline">\(q(\log \sigma) = c\)</span>. These work out to be different priors on <span class="math inline">\(\sigma\)</span> due to the Jacobian adjustment necessary for the log transform.</p>
<p>Stan automatically applies the necessary Jacobian adjustment for variables declared with constraints to ensure a uniform density on the legal constrained values. This Jacobian adjustment is turned off when optimization is being applied in order to produce appropriate maximum likelihood estimates.</p>
</div>
<div id="proper-uniform-priors-interval-constraints" class="section level3 unnumbered">
<h3>Proper Uniform Priors: Interval Constraints</h3>
<p>It is possible to declare a variable with a proper uniform prior by imposing both an upper and lower bound on it, for example,</p>
<pre><code>real&lt;lower=0.1, upper=2.7&gt; sigma;</code></pre>
<p>This will implicitly give <code>sigma</code> a <span class="math inline">\(\mathsf{Uniform}(0.1, 2.7)\)</span> prior.</p>
<div id="matching-support-to-constraints" class="section level4 unnumbered">
<h4>Matching Support to Constraints</h4>
<p>As with all constraints, it is important that the model provide support for all legal values of <code>sigma</code>. For example, the following code constraints <code>sigma</code> to be positive, but then imposes a bounded uniform prior on it.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; sigma;
  ...
model {
  // *** bad *** : support narrower than constraint
  sigma ~ uniform(0.1, 2.7);</code></pre>
<p>The sampling statement imposes a limited support for <code>sigma</code> in (0.1, 2.7), which is narrower than the support declared in the constraint, namely <span class="math inline">\((0, \infty)\)</span>. This can cause the Stan program to be difficult to initialize, hang during sampling, or devolve to a random walk.</p>
</div>
<div id="boundary-estimates" class="section level4 unnumbered">
<h4>Boundary Estimates</h4>
<p>Estimates near boundaries for interval-constrained parameters typically signal that the prior is not appropriate for the model. It can also cause numerical problems with underflow and overflow when sampling or optimizing.</p>
</div>
</div>
<div id="uninformative-proper-priors" class="section level3 unnumbered">
<h3>“Uninformative” Proper Priors</h3>
<p>It is uncommon to see models with priors on regression coefficients such as <span class="math inline">\(\mathsf{normal}(0,1000)\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>If the prior scale, such as 1000, is several orders of magnitude larger than the estimated coefficients, then such a prior is effectively providing no effect whatsoever.</p>
<p>We actively discourage users from using the default scale priors suggested through the BUGS examples <span class="citation">(Lunn et al. <a href="#ref-LunnEtAl:2012">2012</a>)</span>, such as <span class="math display">\[
\sigma^2 \sim \mathsf{InvGamma}(0.001, 0.001).
\]</span></p>
<p>Such priors concentrate too much probability mass outside of reasonable posterior values, and unlike the symmetric wide normal priors, can have the profound effect of skewing posteriors; see <span class="citation">A. Gelman (<a href="#ref-Gelman:2006">2006</a>)</span> for examples and discussion.</p>
</div>
<div id="truncated-priors" class="section level3 unnumbered">
<h3>Truncated Priors</h3>
<p>If a variable is declared with a lower bound of zero, then assigning it a normal prior in a Stan model produces the same effect as providing a properly truncated half-normal prior. The truncation at zero need not be specified as Stan only requires the density up to a proportion. So a variable declared with</p>
<pre><code>real&lt;lower=0&gt; sigma;</code></pre>
<p>and given a prior</p>
<pre><code>sigma ~ normal(0, 1000);</code></pre>
<p>gives <code>sigma</code> a half-normal prior, technically</p>
<p><span class="math display">\[
p(\sigma)
\ = \
\frac{\mathsf{normal}(\sigma | 0, 1000)}
     {1 - \mathsf{NormalCDF}(0 | 0, 1000)}
\ \propto \
\mathsf{normal}(\sigma | 0, 1000),
\]</span></p>
<p>but Stan is able to avoid the calculation of the normal cumulative distribution (CDF) function required to normalize the half-normal density. If either the prior location or scale is a parameter or if the truncation point is a parameter, the truncation cannot be dropped, because the normal CDF term will not be a constant.</p>
</div>
<div id="weakly-informative-priors" class="section level3 unnumbered">
<h3>Weakly Informative Priors</h3>
<p>Typically a researcher will have some knowledge of the scale of the variables being estimated. For instance, if we’re estimating an intercept-only model for the mean population height for adult women, then we know the answer is going to be somewhere in the one to three meter range. That gives us information around which to form a weakly informative prior.</p>
<p>Similarly, a logistic regression with predictors on the standard scale (roughly zero mean, unit variance) is unlikely to have a coefficient that’s larger than five in absolute value. In these cases, it makes sense to provide a weakly informative prior such as <span class="math inline">\(\mathsf{normal}(0,5)\)</span> for such a coefficient.</p>
<p>Weakly informative priors help control inference computationally and statistically. Computationally, a prior increases the curvature around the volume where the solution is expected to lie, which in turn guides both gradient-based like L-BFGS and Hamiltonian Monte Carlo sampling by not allowing them to stray too far from the location of a surface. Statistically, a weakly informative prior is more sensible for a problem like women’s mean height, because a diffuse prior like <span class="math inline">\(\mathsf{normal}(0,1000)\)</span> will ensure that the vast majority of the prior probability mass is outside the range of the expected answer, which can overwhelm the inferences available from a small data set.</p>
</div>
<div id="bounded-priors" class="section level3 unnumbered">
<h3>Bounded Priors</h3>
<p>Consider the women’s height example again. One way to formulate a proper prior is to impose a uniform prior on a bounded scale. For example, we could declare the parameter for mean women’s height to have a lower bound of one meter and an upper bound of three meters. Surely the answer has to lie in that range.</p>
<p>Similarly, it is uncommon to see priors for scale parameters that impose lower bounds of zero and upper bounds of large numbers, such as 10,000.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>This provides roughly the same problem for estimation as a diffuse inverse gamma prior on variance. We prefer to leave parameters which are not absolutely physically constrained to float and provide them informative priors. In the case of women’s height, such a prior might be <span class="math inline">\(\mathsf{normal}(2,0.5)\)</span> on the scale of meters; it concentrates 95% of its mass in the interval <span class="math inline">\((1,3)\)</span>, but still allows values outside of that region.</p>
<p>In cases where bounded priors are used, the posterior fits should be checked to make sure the parameter is not estimated at or close to a boundary. This will not only cause computational problems, it indicates a problem with the way the model is formulated. In such cases, the interval should be widened to see where the parameter fits without such constraints, or boundary-avoid priors should be used (see the <a href="#hierarchical-priors.section">hierarchical priors section</a>.)</p>
</div>
<div id="fat-tailed-priors-and-default-priors" class="section level3 unnumbered">
<h3>Fat-Tailed Priors and Default Priors</h3>
<p>A reasonable alternative if we want to accommodate outliers is to use a prior that concentrates most of mass around the area where values are expected to be, but still leaves a lot of mass in its tails. One choice in such a situation is to use a Cauchy distribution for a prior, which can concentrate its mass around its median, but has tails that are so fat that the variance is infinite.</p>
<p>Without specific information, the Cauchy prior can be a reasonable default parameter choice for regression coefficients <span class="citation">Gelman et al. (<a href="#ref-GelmanJakulinPittauEtAl:2008">2008</a>)</span> and the half-Cauchy (coded implicitly in Stan) a reasonable default choice for scale parameters <span class="citation">A. Gelman (<a href="#ref-Gelman:2006">2006</a>)</span>.</p>
</div>
<div id="informative-priors" class="section level3 unnumbered">
<h3>Informative Priors</h3>
<p>Ideally, there will be substantive information about a problem that can be included in an even tighter prior than a weakly informative prior. This may come from actual prior experiments and thus be the posterior of other data, it may come from meta-analysis, or it may come simply by soliciting it from domain experts. All the goodness of weakly informative priors applies, only with more strength.</p>
</div>
<div id="conjugacy" class="section level3 unnumbered">
<h3>Conjugacy</h3>
<p>Unlike in Gibbs sampling, there is no computational advantage to providing conjugate priors (i.e., priors that produce posteriors in the same family) in a Stan program.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> It is possible in some cases to exploit conjugacy to marginalize out parameters, which can lead to efficiency gains.</p>
<p>Neither the Hamiltonian Monte Carlo samplers or the optimizers make use of conjugacy, working only on the log density and its derivatives.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Gelman:2006">
<p>Gelman, A. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models.” <em>Bayesian Analysis</em> 1 (3): 515–34.</p>
</div>
<div id="ref-ChungEtAl:2013">
<p>Chung, Yeojin, Sophia Rabe-Hesketh, Vincent Dorie, Andrew Gelman, and Jingchen Liu. 2013. “A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models.” <em>Psychometrika</em> 78 (4): 685–709.</p>
</div>
<div id="ref-GelmanJakulinPittauEtAl:2008">
<p>Gelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” <em>Annals of Applied Statistics</em> 2 (4): 1360–83.</p>
</div>
<div id="ref-LunnEtAl:2012">
<p>Lunn, David, Christopher Jackson, Nicky Best, Andrew Thomas, and David Spiegelhalter. 2012. <em>The BUGS Book: A Practical Introduction to Bayesian Analysis</em>. CRC Press/Chapman &amp; Hall.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The practice was common in BUGS and can be seen in most of their examples <span class="citation">Lunn et al. (<a href="#ref-LunnEtAl:2012">2012</a>)</span>.<a href="prior-distributions-and-models-for-data.html#fnref1">↩</a></p></li>
<li id="fn2"><p>This was also a popular strategy in the BUGS example models <span class="citation">(Lunn et al. <a href="#ref-LunnEtAl:2012">2012</a>)</span>, which often went one step further and set the lower bounds to a small number like 0.001 to discourage numerical underflow to zero.<a href="prior-distributions-and-models-for-data.html#fnref2">↩</a></p></li>
<li id="fn3"><p>BUGS and JAGS both support conjugate sampling through Gibbs sampling. JAGS extended the range of conjugacy that could be exploited with its GLM module. Unlike Stan, both BUGS and JAGS are restricted to conjugate priors for constrained multivariate quantities such as covariance matrices or simplexes.<a href="prior-distributions-and-models-for-data.html#fnref3">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fake-data-simulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="some-self-contained-examples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
