<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan</title>
  <meta name="description" content="8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan, with examples and programming techniques.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan, with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan" />
  
  <meta name="twitter:description" content="8 Penalized Maximum Likelihood Point Estimation | Bayesian Workflow Using Stan, with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayesian-data-analysis-1.html">
<link rel="next" href="bayesian-point-estimation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-bayesian-workflow.html#part-1-bayesian-workflow"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Bayesian Workflow</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="fake-data-simulation.html"><a href="fake-data-simulation.html"><i class="fa fa-check"></i><b>2</b> Fake-data Simulation</a></li>
<li class="chapter" data-level="3" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>3</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="4" data-path="some-self-contained-examples.html"><a href="some-self-contained-examples.html"><i class="fa fa-check"></i><b>4</b> Some Self-Contained Examples</a></li>
<li class="chapter" data-level="5" data-path="workflow-in-action.html"><a href="workflow-in-action.html"><i class="fa fa-check"></i><b>5</b> Workflow in Action</a></li>
<li class="chapter" data-level="6" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>6</b> Modeling as Software Development</a></li>
<li><a href="part-2-bayesian-inference.html#part-2-bayesian-inference"><i style="font-size: 110%; color:#990017;">Part 2: Bayesian Inference</i></a></li>
<li class="chapter" data-level="7" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>7</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="8" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>8</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="9" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="10" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>10</b> Variational Inference</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Workflow Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mle.chapter" class="section level1">
<h1><span class="header-section-number">8</span> Penalized Maximum Likelihood Point Estimation</h1>
<p>This chapter defines the workhorses of non-Bayesian estimation, maximum likelihood and penalized maximum likelihood, and relates them to Bayesian point estimation based on posterior means, medians, and modes. Such estimates are called “point estimates” because they are composed of a single value for the model parameters <span class="math inline">\(\theta\)</span> rather than a posterior distribution.</p>
<p>Stan’s optimizer can be used to implement (penalized) maximum likelihood estimation for any likelihood function and penalty function that can be coded in Stan’s modeling language. Stan’s optimizer can also be used for point estimation in Bayesian settings based on posterior modes. Stan’s Markov chain Monte Carlo samplers can be used to implement point inference in Bayesian models based on posterior means or medians.</p>
<div id="mle.section" class="section level2">
<h2><span class="header-section-number">8.1</span> Maximum Likelihood Estimation</h2>
<p>Given a likelihood function <span class="math inline">\(p(y|\theta)\)</span> and a fixed data vector <span class="math inline">\(y\)</span>, the maximum likelihood estimate (MLE) is the parameter vector <span class="math inline">\(\hat{\theta}\)</span> that maximizes the likelihood, i.e., <span class="math display">\[
\hat{\theta} = \mbox{argmax}_{\theta} \ p(y|\theta).
\]</span> It is usually more convenient to work on the log scale. The following is an equivalent formulateion of the MLE,<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p><span class="math display">\[
\hat{\theta} = \mbox{argmax}_{\theta} \ \log p(y|\theta).
\]</span></p>
<div id="existence-of-maximum-likelihood-estimates" class="section level3 unnumbered">
<h3>Existence of Maximum Likelihood Estimates</h3>
<p>Because not all functions have unique maximum values, maximum likelihood estimates are not guaranteed to exist. As discussed in the <a href="#problematic-posteriors.chapter">problematic postriors chapter</a>, this situation can arise when</p>
<ul>
<li>there is more than one point that maximizes the likelihood function,</li>
<li>the likelihood function is unbounded, or</li>
<li>the likelihood function is bounded by an asymptote that is never reached for legal parameter values.</li>
</ul>
<p>These problems persist with the penalized maximum likelihood estimates discussed in the next section, and Bayesian posterior modes as discussed in the following section.</p>
</div>
<div id="example-linear-regression" class="section level3 unnumbered">
<h3>Example: Linear Regression</h3>
<p>Consider an ordinary linear regression problem with an <span class="math inline">\(N\)</span>-dimensional vector of observations <span class="math inline">\(y\)</span>, an <span class="math inline">\((N \times K)\)</span>-dimensional data matrix <span class="math inline">\(x\)</span> of predictors, a <span class="math inline">\(K\)</span>-dimensional parameter vector <span class="math inline">\(\beta\)</span> of regression coefficients, and a real-valued noise scale <span class="math inline">\(\sigma &gt; 0\)</span>, with log likelihood function <span class="math display">\[
\log p(y|\beta,x) = \sum_{n=1}^N \log \mathsf{normal}(y_n|x_n \beta,
\sigma).
\]</span></p>
<p>The maximum likelihood estimate for <span class="math inline">\(\theta = (\beta,\sigma)\)</span> is just <span class="math display">\[
(\hat{\beta},\hat{\sigma})
\ = \
\mbox{argmax}_{\beta,\sigma}
\log p(y|\beta,\sigma,x) = \sum_{n=1}^N \log \mathsf{normal}(y_n|x_n \beta, \sigma).
\]</span></p>
<div id="squared-error" class="section level4 unnumbered">
<h4>Squared Error</h4>
<p>A little algebra on the log likelihood function shows that the marginal maximum likelihood estimate <span class="math inline">\(\hat{\theta} = (\hat{\beta},\hat{\sigma})\)</span> can be equivalently formulated for <span class="math inline">\(\hat{\beta}\)</span> in terms of least squares. That is, <span class="math inline">\(\hat{\beta}\)</span> is the value for the coefficient vector that minimizes the sum of squared prediction errors,</p>
<p><span class="math display">\[
\hat{\beta}
\ = \
\mbox{argmin}_{\beta} \sum_{n=1}^N (y_n - x_n \beta)^2
\ = \
\mbox{argmin}_{\beta} (y - x \beta)^{\top} (y - x\beta).
\]</span></p>
<p>The residual error for data item <span class="math inline">\(n\)</span> is the difference between the actual value and predicted value, <span class="math inline">\(y_n - x_n \hat{\beta}\)</span>. The maximum likelihood estimate for the noise scale, <span class="math inline">\(\hat{\sigma}\)</span> is just the square root of the average squared residual, <span class="math display">\[
\hat{\sigma}^2
\ = \
\frac{1}{N} \sum_{n=1}^N \left( y_n - x_n \hat{\beta} \right)^2
\ = \
\frac{1}{N} (y - x \hat{\beta})^{\top} (y - x\hat{\beta}).
\]</span></p>
</div>
<div id="minimizing-squared-error-in-stan" class="section level4 unnumbered">
<h4>Minimizing Squared Error in Stan</h4>
<p>The squared error approach to linear regression can be directly coded in Stan with the following model.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; K;
  vector[N] y;
  matrix[N,K] x;
}
parameters {
  vector[K] beta;
}
transformed parameters {
  real&lt;lower=0&gt; squared_error;
  squared_error = dot_self(y - x * beta);
}
model {
  target += -squared_error;
}
generated quantities {
  real&lt;lower=0&gt; sigma_squared;
  sigma_squared = squared_error / N;
}</code></pre>
<p>Running Stan’s optimizer on this model produces the MLE for the linear regression by directly minimizing the sum of squared errors and using that to define the noise scale as a generated quantity.</p>
<p>By replacing <code>N</code> with <code>N-1</code> in the denominator of the definition of <code>sigma_squared</code>, the more commonly supplied unbiased estimate of <span class="math inline">\(\sigma^2\)</span> can be calculated; see the <a href="mle-chapter.html#estimation-bias.section">estimation bias section</a> for a definition and a discussion of estimating variance.</p>
</div>
</div>
</div>
<div id="penalized-maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">8.2</span> Penalized Maximum Likelihood Estimation</h2>
<p>There is nothing special about a likelihood function as far as the ability to perform optimization is concerned. It is common among non-Bayesian statisticians to add so-called “penalty” functions to log likelihoods and optimize the new function. The penalized maximum likelihood estimator for a log likelihood function <span class="math inline">\(\log p(y|\theta)\)</span> and penalty function <span class="math inline">\(r(\theta)\)</span> is defined to be <span class="math display">\[
\hat{\theta} = \mbox{argmax}_{\theta} \log p(y|\theta) - r(\theta).
\]</span> The penalty function <span class="math inline">\(r(\theta)\)</span> is negated in the maximization so that the estimate <span class="math inline">\(\hat{\theta}\)</span> balances maximizing the log likelihood and minimizing the penalty. Penalization is sometimes called “regularization.”</p>
<div id="penalized-mle-examples" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Examples {-}</h3>
<div id="ridge-regression" class="section level4 unnumbered">
<h4>Ridge Regression</h4>
<p>Ridge regression <span class="citation">Hoerl and Kennard (<a href="#ref-HoerlKennard:1970">1970</a>)</span> is based on penalizing the Euclidean length of the coefficient vector <span class="math inline">\(\beta\)</span>. The ridge penalty function is</p>
<p><span class="math display">\[
r(\beta)
\ = \
\lambda \, \sum_{k=1}^K \beta_k^2
\ = \
\lambda \, \beta^{\top} \beta,
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a constant tuning parameter that determines the magnitude of the penalty.</p>
<p>Therefore, the penalized maximum likelihood estimate for ridge regression is just</p>
<p><span class="math display">\[
(\hat{\beta},\hat{\sigma})
\ = \
\mbox{argmax}_{\beta,\sigma} \,
 \sum_{n=1}^N \log \mathsf{normal}(y_n|x_n \beta, \sigma) - \lambda
 \sum_{k=1}^K \beta_k^2
\]</span></p>
<p>The ridge penalty is sometimes called L2 regularization or shrinkage, because of its relation to the L2 norm.</p>
<p>Like the basic MLE for linear regression, the ridge regression estimate for the coefficients <span class="math inline">\(\beta\)</span> can also be formulated in terms of least squares,</p>
<p><span class="math display">\[
\hat{\beta}
\ = \
\mbox{argmin}_{\beta} \, \sum_{n=1}^N (y_n - x_n \beta)^2 + \sum_{k=1}^K \beta_k^2
\ = \
\mbox{argmin}_{\beta} \, (y - x\beta)^{\top} (y - x\beta) +
\lambda \beta^{\top} \beta.
\]</span></p>
<p>The effect of adding the ridge penalty function is that the ridge regression estimate for <span class="math inline">\(\beta\)</span> is a vector of shorter length, or in other words, <span class="math inline">\(\hat{\beta}\)</span> is shrunk. The ridge estimate does not necessarily have a smaller absolute <span class="math inline">\(\beta_k\)</span> for each <span class="math inline">\(k\)</span>, nor does the coefficient vector necessarily point in the same direction as the maximum likelihood estimate.</p>
<p>In Stan, adding the ridge penalty involves adding its magnitude as a data variable and the penalty itself to the model block,</p>
<pre><code>data {
  // ...
  real&lt;lower=0&gt; lambda;
}
// ...
model {
  // ...
  target += - lambda * dot_self(beta);
}</code></pre>
<p>The noise term calculation remains the same.</p>
</div>
<div id="lasso" class="section level4 unnumbered">
<h4>Lasso</h4>
<p>Lasso <span class="citation">(Tibshirani <a href="#ref-Tibshirani:1996">1996</a>)</span> is an alternative to ridge regression that applies a penalty based on the sum of the absolute coefficients, rather than the sum of their squares, <span class="math display">\[
r(\beta) = \lambda \sum_{k=1}^K | \beta_k |.
\]</span> Lasso is also called L1 shrinkage due to its relation to the L1 norm, which is also known as taxicab distance or Manhattan distance.</p>
<p>Because the derivative of the penalty does not depend on the value of the <span class="math inline">\(\beta_k\)</span>, <span class="math display">\[
\frac{d}{d\beta_k} \lambda \sum_{k=1}^K | \beta_k | =
\mbox{signum}(\beta_k),
\]</span> it has the effect of shrinking parameters all the way to 0 in maximum likelihood estimates. Thus it can be used for variable selection as well as just shrinkage.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>Lasso can be implemented in Stan just as easily as ridge regression, with the magnitude declared as data and the penalty added to the model block,</p>
<pre><code>data {
  // ...
  real&lt;lower=0&gt; lambda;
}
// ...
model {
  // ...
  for (k in 1:K)
    target += - lambda * fabs(beta[k]);
}</code></pre>
</div>
<div id="elastic-net" class="section level4 unnumbered">
<h4>Elastic Net</h4>
<p>The naive elastic net <span class="citation">(Zou and Hastie <a href="#ref-ZouHastie:2005">2005</a>)</span> involves a weighted average of ridge and lasso penalties, with a penalty function <span class="math display">\[
r(\beta)
= \lambda_1 \sum_{k=1}^K |\beta_k|
+ \lambda_2 \sum_{k=1}^K \beta_k^2.
\]</span> The naive elastic net combines properties of both ridge regression and lasso, providing both identification and variable selection.</p>
<p>The naive elastic net can be implemented directly in Stan by combining implementations of ridge regression and lasso, as</p>
<pre><code>data {
  real&lt;lower=0&gt; lambda1;
  real&lt;lower=0&gt; lambda2;
  // ...
}
// ...
model {
  // ...
  for (k in 1:K)
    target += -lambda1 * fabs(beta[k]);
  target += -lambda2 * dot_self(beta);
}</code></pre>
<p>The signs are negative in the program because <span class="math inline">\(r(\beta)\)</span> is a penalty function.</p>
<p>The elastic net <span class="citation">(Zou and Hastie <a href="#ref-ZouHastie:2005">2005</a>)</span> involves adjusting the final estimate for <span class="math inline">\(\beta\)</span> based on the fit <span class="math inline">\(\hat{\beta}\)</span> produced by the naive elastic net. The elastic net estimate is <span class="math display">\[
\hat{\beta} = (1 + \lambda_2) \beta^*
\]</span> where <span class="math inline">\(\beta^{*}\)</span> is the naive elastic net estimate.</p>
<p>To implement the elastic net in Stan, the data, parameter, and model blocks are the same as for the naive elastic net. In addition, the elastic net estimate is calculated in the generated quantities block.</p>
<pre><code>generated quantities {
  vector[K] beta_elastic_net;
  // ...
  beta_elastic_net = (1 + lambda2) * beta;
}</code></pre>
<p>The error scale also needs to be calculated in the generated quantities block based on the elastic net coefficients <code>beta_elastic_net</code>.</p>
</div>
<div id="other-penalized-regressions" class="section level4 unnumbered">
<h4>Other Penalized Regressions</h4>
<p>It is also common to use penalty functions that bias the coefficient estimates toward values other than 0, as in the estimators of <span class="citation">James and Stein (<a href="#ref-JamesStein:1961">1961</a>)</span>. Penalty functions can also be used to bias estimates toward population means; see <span class="citation">B. Efron and Morris (<a href="#ref-EfronMorris:1975">1975</a>)</span> and <span class="citation">Bradley Efron (<a href="#ref-Efron:2012">2012</a>)</span>. This latter approach is similar to the hierarchical models commonly employed in Bayesian statistics.</p>
</div>
</div>
</div>
<div id="estimation-bias.section" class="section level2">
<h2><span class="header-section-number">8.3</span> Estimation Error, Bias, and Variance</h2>
<p>An estimate <span class="math inline">\(\hat{\theta}\)</span> depends on the particular data <span class="math inline">\(y\)</span> and either the log likelihood function, <span class="math inline">\(\log p(y|\theta)\)</span>, penalized log likelihood function <span class="math inline">\(\log p(y|\theta) - r(\theta)\)</span>, or log probability function <span class="math inline">\(\log p(y,\theta) = \log p(y,\theta) + \log p(\theta)\)</span>. In this section, the notation <span class="math inline">\(\hat{\theta}\)</span> is overloaded to indicate the estimator, which is an implicit function of the data and (penalized) likelihood or probability function.</p>
<div id="estimation-error" class="section level3 unnumbered">
<h3>Estimation Error</h3>
<p>For a particular observed data set <span class="math inline">\(y\)</span> generated according to true parameters <span class="math inline">\(\theta\)</span>, the estimation error is the difference between the estimated value and true value of the parameter, <span class="math display">\[
\mbox{err}(\hat{\theta}) = \hat{\theta} - \theta.
\]</span></p>
</div>
<div id="estimation-bias" class="section level3 unnumbered">
<h3>Estimation Bias</h3>
<p>For a particular true parameter value <span class="math inline">\(\theta\)</span> and a likelihood function <span class="math inline">\(p(y|\theta)\)</span>, the expected estimation error averaged over possible data sets <span class="math inline">\(y\)</span> according to their density under the likelihood is</p>
<p><span class="math display">\[
\mathbb{E}_{p(y|\theta)}[\hat{\theta}]
\ = \
\int \left( \mbox{argmax}_{\theta&#39;} p(y|\theta&#39;) \right) p(y|\theta) dy.
\]</span></p>
<p>An estimator’s bias is the expected estimation error,</p>
<p><span class="math display">\[
\mathbb{E}_{p(y|\theta)}[\hat{\theta} - \theta]
\ = \
\mathbb{E}_{p(y|\theta)}[\hat{\theta}] - \theta
\]</span></p>
<p>The bias is a multivariate quantity with the same dimensions as <span class="math inline">\(\theta\)</span>. An estimator is unbiased if its expected estimation error is zero and biased otherwise.</p>
<div id="example-estimating-a-normal-distribution" class="section level4 unnumbered">
<h4>Example: Estimating a Normal Distribution</h4>
<p>Suppose a data set of observations <span class="math inline">\(y_n\)</span> for <span class="math inline">\(n \in 1{:}N\)</span> drawn from a normal distribution. This presupposes a model <span class="math inline">\(y_n \sim \mathsf{normal}(\mu,\sigma)\)</span>, where both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma &gt; 0\)</span> are parameters. The log likelihood is just <span class="math display">\[
\log p(y|\mu,\sigma) = \sum_{n=1}^N \log
\mathsf{normal}(y_n|\mu,\sigma).
\]</span> The maximum likelihood estimator for <span class="math inline">\(\mu\)</span> is just the sample mean, i.e., the average of the samples, <span class="math display">\[
\hat{\mu} = \frac{1}{N} \sum_{n=1}^N y_n.
\]</span> The maximum likelihood estimate for the mean is unbiased.</p>
<p>The maximum likelihood estimator for the variance <span class="math inline">\(\sigma^2\)</span> is the average of the squared difference from the mean, <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{N} \sum_{n=1}^N (y_n - \hat{\mu})^2.
\]</span> The maximum likelihood for the variance is biased on the low side, i.e.,</p>
<p><span class="math display">\[
\mathbb{E}_{p(y|\mu,\sigma)}[\hat{\sigma}^2] &lt; \sigma.
\]</span></p>
<p>The reason for this bias is that the maximum likelihood estimate is based on the difference from the estimated mean <span class="math inline">\(\hat{\mu}\)</span>. Plugging in the actual mean can lead to larger sum of squared differences; if <span class="math inline">\(\mu \neq \hat{\mu}\)</span>, then <span class="math display">\[
\frac{1}{N} \sum_{n=1}^N (y_n - \mu)^2
&gt;
\frac{1}{N} \sum_{n=1}^N (y_n - \hat{\mu})^2.
\]</span></p>
<p>An alternative estimate for the variance is the sample variance, which is defined by <span class="math display">\[
\hat{\mu} = \frac{1}{N-1} \sum_{n=1}^N (y_n - \hat{\mu})^2.
\]</span> This value is larger than the maximum likelihood estimate by a factor of <span class="math inline">\(N/(N-1)\)</span>.</p>
</div>
</div>
<div id="estimation-variance" class="section level3 unnumbered">
<h3>Estimation Variance</h3>
<p>The variance of component <span class="math inline">\(k\)</span> of an estimator <span class="math inline">\(\hat{\theta}\)</span> is computed like any other variance, as the expected squared difference from its expectation,</p>
<p><span class="math display">\[
\mbox{var}_{p(y|\theta})[\hat{\theta}_k]
\ = \
\mathbb{E}_{p(y|\theta})[\, (\hat{\theta}_k -
\mathbb{E}_{p(y|\theta)}[\hat{\theta}_k])^2 \,].
\]</span></p>
<p>The full <span class="math inline">\(K \times K\)</span> covariance matrix for the estimator is thus defined, as usual, by</p>
<p><span class="math display">\[
\mbox{covar}_{p(y|\theta)}[\hat{\theta}]
\ = \
\mathbb{E}_{p(y|\theta})[\, (\hat{\theta} - \mathbb{E}[\hat{\theta}]) \,
                         (\hat{\theta} -
                         \mathbb{E}[\hat{\theta}])^{\top} \, ].
\]</span></p>
<p>Continuing the example of estimating the mean and variance of a normal distribution based on sample data, the maximum likelihood estimator (in this case, the sample mean) is the unbiased estimator for the mean <span class="math inline">\(\mu\)</span> with the lowest variance; the Gauss-Markov theorem establishes this result in some generality for least-squares estimation, or equivalently, maximum likelihood estimation under an assumption of normal noise; see @[Section~3.2.2]{HastieTibshiraniFriedman:2009}.</p>

</div>
</div>
</div>
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-HoerlKennard:1970">
<p>Hoerl, Arthur E., and Robert W. Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” <em>Technometrics</em> 12 (1): 55–67.</p>
</div>
<div id="ref-Tibshirani:1996">
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society, Series B</em> 58 (1): 267–88.</p>
</div>
<div id="ref-ZouHastie:2005">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society, Series B</em> 67 (2): 301–20.</p>
</div>
<div id="ref-JamesStein:1961">
<p>James, W., and Charles Stein. 1961. “Estimation with Quadratic Loss.” In <em>Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability</em>, edited by Jerzey Neyman, 1:361–79. University of California Press.</p>
</div>
<div id="ref-EfronMorris:1975">
<p>Efron, B., and C. Morris. 1975. “Data Analysis Using Stein’s Estimator and Its Generalizations.” <em>Journal of the American Statistical Association</em> 70: 311–19.</p>
</div>
<div id="ref-Efron:2012">
<p>Efron, Bradley. 2012. <em>Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction</em>. Institute of Mathematical Statistics Monographs. Cambridge Univesity Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>The equivalence follows from the fact that densities are positive and the log function is strictly monotonic, i.e., <span class="math inline">\(p(y|\theta) \geq 0\)</span> and for all <span class="math inline">\(a, b &gt; 0\)</span>, <span class="math inline">\(\log a &gt; \log b\)</span> if and only if <span class="math inline">\(a &gt; b\)</span>.<a href="mle-chapter.html#fnref8">↩</a></p></li>
<li id="fn9"><p>In practice, Stan’s gradient-based optimizers are not guaranteed to produce exact zero values; see <span class="citation">Langford, Li, and Zhang (<a href="#ref-LangfordEtAl:2009">2009</a>)</span> for a discussion of getting exactly zero values with gradient descent.<a href="mle-chapter.html#fnref9">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-data-analysis-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-point-estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
